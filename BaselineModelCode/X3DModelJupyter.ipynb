{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6323d5",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c4d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import torch.utils.data\n",
    "import pytorchvideo.data\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Apply a transform to a dict key (like ApplyTransformToKey)\n",
    "# =========================\n",
    "\n",
    "class ApplyToKey:\n",
    "    def __init__(self, key, transform):\n",
    "        self.key = key\n",
    "        self.transform = transform\n",
    "    def __call__(self, x):\n",
    "        x[self.key] = self.transform(x[self.key])\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Temporal subsample on (C, T, H, W)\n",
    "class UniformTemporalSubsample:\n",
    "    def __init__(self, num_samples: int):\n",
    "        self.num_samples = num_samples\n",
    "    def __call__(self, video: torch.Tensor) -> torch.Tensor:\n",
    "        C, T, H, W = video.shape\n",
    "        if T <= self.num_samples:\n",
    "            # If clip is short, repeat last frame to reach num_samples (safer than failing)\n",
    "            idx = torch.arange(T)\n",
    "            if T < self.num_samples:\n",
    "                pad = idx.new_full((self.num_samples - T,), T - 1)\n",
    "                idx = torch.cat([idx, pad], dim=0)\n",
    "            return video[:, idx, :, :]\n",
    "        idx = torch.linspace(0, T - 1, self.num_samples).long()\n",
    "        return video[:, idx, :, :]\n",
    "\n",
    "# Normalize (C, T, H, W) using TF.normalize frame-by-frame\n",
    "class VideoNormalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, video: torch.Tensor) -> torch.Tensor:\n",
    "        frames = [TF.normalize(video[:, t], self.mean, self.std) for t in range(video.shape[1])]\n",
    "        return torch.stack(frames, dim=1)\n",
    "\n",
    "# Resize each frame so short side = s (random range for train, fixed for val)\n",
    "class RandomShortSideScale:\n",
    "    def __init__(self, min_size=256, max_size=320):\n",
    "        self.min_size = min_size\n",
    "        self.max_size = max_size\n",
    "    def __call__(self, video: torch.Tensor) -> torch.Tensor:\n",
    "        s = int(torch.randint(self.min_size, self.max_size + 1, (1,)).item())\n",
    "        return torch.stack([TF.resize(video[:, t], s) for t in range(video.shape[1])], dim=1)\n",
    "\n",
    "class ShortSideScale:\n",
    "    def __init__(self, size=256):\n",
    "        self.size = size\n",
    "    def __call__(self, video: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.stack([TF.resize(video[:, t], self.size) for t in range(video.shape[1])], dim=1)\n",
    "\n",
    "class RandomCrop:\n",
    "    def __init__(self, size=224):\n",
    "        self.size = size\n",
    "    def __call__(self, video: torch.Tensor) -> torch.Tensor:\n",
    "        _, _, H, W = video.shape\n",
    "        th, tw = self.size, self.size\n",
    "        if H < th or W < tw:\n",
    "            # If small, pad (rare, but avoids crashing)\n",
    "            pad_h = max(0, th - H)\n",
    "            pad_w = max(0, tw - W)\n",
    "            video = torch.nn.functional.pad(video, (0, pad_w, 0, pad_h))\n",
    "            _, _, H, W = video.shape\n",
    "        i = int(torch.randint(0, H - th + 1, (1,)).item())\n",
    "        j = int(torch.randint(0, W - tw + 1, (1,)).item())\n",
    "        return video[:, :, i:i+th, j:j+tw]\n",
    "\n",
    "class CenterCrop:\n",
    "    def __init__(self, size=224):\n",
    "        self.size = size\n",
    "    def __call__(self, video: torch.Tensor) -> torch.Tensor:\n",
    "        _, _, H, W = video.shape\n",
    "        th, tw = self.size, self.size\n",
    "        i = max(0, (H - th) // 2)\n",
    "        j = max(0, (W - tw) // 2)\n",
    "        return video[:, :, i:i+th, j:j+tw]\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "    def __call__(self, video: torch.Tensor) -> torch.Tensor:\n",
    "        if torch.rand(1).item() < self.p:\n",
    "            return torch.flip(video, dims=[3])  # flip width\n",
    "        return video\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "    def __call__(self, x):\n",
    "        for t in self.transforms:\n",
    "            x = t(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da108e4",
   "metadata": {},
   "source": [
    "0) Data Loading Roots (OneDrive approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_ROOT = Path(os.environ[\"WINTERLAB_VIDEO_ROOT\"])\n",
    "SPLIT_ROOT = Path(os.environ[\"WINTERLAB_SPLIT_ROOT\"])\n",
    "\n",
    "assert VIDEO_ROOT.exists(), f\"WINTERLAB_VIDEO_ROOT does not exist: {VIDEO_ROOT}\"\n",
    "assert (SPLIT_ROOT / \"train.csv\").exists(), f\"train.csv not found in WINTERLAB_SPLIT_ROOT: {SPLIT_ROOT}\"\n",
    "assert (SPLIT_ROOT / \"val.csv\").exists(), f\"val.csv not found in WINTERLAB_SPLIT_ROOT: {SPLIT_ROOT}\"\n",
    "assert (SPLIT_ROOT / \"test.csv\").exists(), f\"test.csv not found in WINTERLAB_SPLIT_ROOT: {SPLIT_ROOT}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc594f",
   "metadata": {},
   "source": [
    "Load labeled paths from split CSV - CSV format expected: columns [\"path\", \"label\"] where \"path\" is RELATIVE to VIDEO_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ec5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labeled_video_paths(split_csv_path: Path):\n",
    "    df = pd.read_csv(split_csv_path)\n",
    "\n",
    "    if \"path\" not in df.columns or \"label\" not in df.columns:\n",
    "        raise ValueError(f\"{split_csv_path} must contain columns: 'path' and 'label'\")\n",
    "\n",
    "    labeled = []\n",
    "    for _, row in df.iterrows():\n",
    "        rel_path = str(row[\"path\"])\n",
    "        label = int(row[\"label\"])\n",
    "\n",
    "        full_path = (VIDEO_ROOT / rel_path).as_posix()\n",
    "\n",
    "        # IMPORTANT: label must be a mapping/dict for this PyTorchVideo version\n",
    "        labeled.append((full_path, {\"label\": label}))\n",
    "\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be28edc2",
   "metadata": {},
   "source": [
    "2) Transforms (Tune later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FRAMES = 16\n",
    "CROP_SIZE = 224\n",
    "CLIP_DURATION = 2.0\n",
    "\n",
    "train_transform = Compose([\n",
    "    ApplyToKey(\"video\", Compose([\n",
    "        UniformTemporalSubsample(NUM_FRAMES),\n",
    "        lambda v: v / 255.0,\n",
    "        VideoNormalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "        RandomShortSideScale(256, 320),\n",
    "        RandomCrop(CROP_SIZE),\n",
    "        RandomHorizontalFlip(0.5),\n",
    "    ]))\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    ApplyToKey(\"video\", Compose([\n",
    "        UniformTemporalSubsample(NUM_FRAMES),\n",
    "        lambda v: v / 255.0,\n",
    "        VideoNormalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "        ShortSideScale(256),\n",
    "        CenterCrop(CROP_SIZE),\n",
    "    ]))\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    ApplyToKey(\"video\", Compose([\n",
    "        UniformTemporalSubsample(NUM_FRAMES),\n",
    "        lambda v: v / 255.0,\n",
    "        VideoNormalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "        ShortSideScale(256),\n",
    "        CenterCrop(CROP_SIZE),\n",
    "    ]))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90b869d",
   "metadata": {},
   "source": [
    "3) Make PyTorchVideo dataset: Train uses random clip sampler, val/test use uniform sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a236485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(labeled_video_paths, split: str):\n",
    "    if split not in {\"train\", \"val\", \"test\"}:\n",
    "        raise ValueError(\"split must be one of: 'train', 'val', 'test'\")\n",
    "\n",
    "    clip_sampler = pytorchvideo.data.make_clip_sampler(\n",
    "        \"random\" if split == \"train\" else \"uniform\",\n",
    "        CLIP_DURATION\n",
    "    )\n",
    "\n",
    "    transform = train_transform if split == \"train\" else val_transform if split == \"val\" else test_transform\n",
    "\n",
    "    return pytorchvideo.data.LabeledVideoDataset(\n",
    "        labeled_video_paths=labeled_video_paths,  # list[(video_path_str, label_int)]\n",
    "        clip_sampler=clip_sampler,\n",
    "        decode_audio=False,\n",
    "        transform=transform,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6bcc32",
   "metadata": {},
   "source": [
    "4) Build datasets + dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df564b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0  ##changed from 4 to 0 in order for me to run this\n",
    "\n",
    "train_labeled = load_labeled_video_paths(SPLIT_ROOT / \"train.csv\")\n",
    "val_labeled   = load_labeled_video_paths(SPLIT_ROOT / \"val.csv\")\n",
    "test_labeled  = load_labeled_video_paths(SPLIT_ROOT / \"test.csv\")\n",
    "\n",
    "train_ds = make_dataset(train_labeled, split=\"train\")\n",
    "val_ds   = make_dataset(val_labeled, split=\"val\")\n",
    "test_ds  = make_dataset(test_labeled, split=\"test\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "val_loader   = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "test_loader  = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a3e1c",
   "metadata": {},
   "source": [
    "5) Sanity check one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f0f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch keys:\", batch.keys())\n",
    "print(\"video shape:\", batch[\"video\"].shape)   # (B, C, T, H, W)\n",
    "print(\"label shape:\", batch[\"label\"].shape)   # (B,)\n",
    "print(\"labels:\", batch[\"label\"])\n",
    "print(\"video_name (first 2):\", batch[\"video_name\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a5c4a",
   "metadata": {},
   "source": [
    "# LOAD PRE-TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ece532",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- Load pretrained X3D-S ----\n",
    "# This uses torch.hub to fetch the model definition + weights.\n",
    "# If this fails due to network restrictions, set pretrained=False.\n",
    "model = torch.hub.load(\n",
    "    \"facebookresearch/pytorchvideo\",\n",
    "    \"x3d_s\",\n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "num_classes = 2 #two-foot slip vs no slip\n",
    "in_features = model.blocks[-1].proj.in_features\n",
    "model.blocks[-1].proj = nn.Linear(in_features, num_classes) #overwrite previous classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2862f6",
   "metadata": {},
   "source": [
    "Sanity Check forward pass to confirm that the model, data, and GPU setup work before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd120f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.train()  # training mode\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "video = batch[\"video\"].to(device)   # (B, C, T, H, W)\n",
    "label = batch[\"label\"].to(device)   # (B,)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(video)\n",
    "\n",
    "print(\"logits shape:\", logits.shape)   # expect: (B, 2)\n",
    "print(\"labels shape:\", label.shape)    # expect: (B,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d326ff",
   "metadata": {},
   "source": [
    "Loss + Optimizer - Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519243ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\")) # mixed precision on GPU\n",
    "\n",
    "# Training Loop - Accuracy is Clip-level here\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        video = batch[\"video\"].to(device, non_blocking=True)\n",
    "        label = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True) #reset gradients before computing new ones\n",
    "\n",
    "        # Mixed precision speeds up on GPU\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == \"cuda\")):\n",
    "            logits = model(video)\n",
    "            loss = criterion(logits, label)\n",
    "\n",
    "        if device.type == \"cuda\": #GPU\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else: #CPU\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * video.size(0) #Update Metrics\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == label).sum().item()\n",
    "        total += label.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_video_level_max(model, loader, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes video-level accuracy by taking the max slip probability\n",
    "    across all clips belonging to the same video.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    max_prob = defaultdict(float)   # video_index -> max P(slip)\n",
    "    true_label = {}                 # video_index -> ground truth label\n",
    "\n",
    "    for batch in loader:\n",
    "        video = batch[\"video\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"label\"].cpu()\n",
    "        vid_idx = batch[\"video_index\"].cpu().tolist()\n",
    "\n",
    "        logits = model(video)\n",
    "        probs = F.softmax(logits, dim=1)[:, 1].cpu()  # P(slip)\n",
    "\n",
    "        for i, v in enumerate(vid_idx):\n",
    "            max_prob[v] = max(max_prob[v], float(probs[i]))\n",
    "            true_label[v] = int(labels[i])\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for v, p in max_prob.items():\n",
    "        pred = 1 if p >= threshold else 0\n",
    "        correct += (pred == true_label[v])\n",
    "        total += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff5276",
   "metadata": {},
   "source": [
    "Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec58d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Loop\n",
    "# =========================\n",
    "@torch.no_grad() #no gradients are stores\n",
    "def Printeval_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        video = batch[\"video\"].to(device, non_blocking=True)\n",
    "        label = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == \"cuda\")):\n",
    "            logits = model(video)\n",
    "            loss = criterion(logits, label)\n",
    "\n",
    "        total_loss += loss.item() * video.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == label).sum().item()\n",
    "        total += label.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db91f86",
   "metadata": {},
   "source": [
    "Run N Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59664344",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    \n",
    "    # 1) Train (clip-level training)\n",
    "    train_loss, train_clip_acc = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, device\n",
    "    )\n",
    "\n",
    "    \n",
    "    #Train video-level accuracy (sanity check)\n",
    "    train_video_acc = eval_video_level_max(\n",
    "        model, train_loader, device\n",
    "    )\n",
    "\n",
    "    # 3) Validation: clip-level proxy + video-level real metric\n",
    "    val_loss, val_clip_acc = Printeval_one_epoch(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    val_video_acc = eval_video_level_max(\n",
    "        model, val_loader, device\n",
    "    )\n",
    "\n",
    "    # 4) Print metrics\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"Train loss {train_loss:.4f} CLIP acc {train_clip_acc:.4f} VIDEO acc {train_video_acc:.4f} | \"\n",
    "        f\"Val loss {val_loss:.4f} CLIP acc {val_clip_acc:.4f} VIDEO acc {val_video_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# 5) Save final checkpoint\n",
    "# -------------------------\n",
    "torch.save(model.state_dict(), \"x3d_model.pth\")\n",
    "print(\"Training done, model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd495a81",
   "metadata": {},
   "source": [
    "Save Training Model State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce01e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"x3d_model.pth\", map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac35ac0",
   "metadata": {},
   "source": [
    "Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cdcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Loop at a Clip Level\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def test_clip_level(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        video = batch[\"video\"].to(device, non_blocking=True)\n",
    "        label = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == \"cuda\")):\n",
    "            logits = model(video)\n",
    "            loss = criterion(logits, label)\n",
    "\n",
    "        total_loss += loss.item() * video.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == label).sum().item()\n",
    "        total += label.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "# ---- Final test (RUN ONCE) ----\n",
    "test_loss, test_clip_acc = test_clip_level(model, test_loader, criterion, device)\n",
    "test_video_acc = eval_video_level_max(model, test_loader, device)\n",
    "\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(f\"Clip-level accuracy:  {test_clip_acc:.4f}\")\n",
    "print(f\"Video-level accuracy: {test_video_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
